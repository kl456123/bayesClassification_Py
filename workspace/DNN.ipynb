{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculation loss function not in top layer\n",
    "I just want to run dnn in mnist dataset to test my idea.\n",
    "\n",
    "I choose mnist due to its not too easy and not too complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first read data from mnist dataset\n",
    "def readDataset(filename_queue):\n",
    "    tf.train.string_input_producer(filename_queue)\n",
    "    reader = tf.TextLineReader()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting dataset/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'dataset/mnist'\n",
    "# the datasets include traindata and testdata,\n",
    "# so get traindata by using datasets.train for example.\n",
    "# And get other dataset like before .\n",
    "datasets = input_data.read_data_sets(train_dir=train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def onelayer(inputs,layername,w_initial_value,b_initial_value,i):\n",
    "    with tf.variable_scope(layername):\n",
    "#         a = f(W*x+b) f refers to ReLU \n",
    "#         W = tf.Variable(name='weights',initial_value=w_initial_value,dtype=tf.float32)\n",
    "        weights = tf.Variable(\n",
    "        w_initial_value,\n",
    "        name='weights')\n",
    "#     record weights\n",
    "        tf.histogram_summary('weights'+str(i),weights)\n",
    "        \n",
    "        biases = tf.Variable(name='biases',initial_value=b_initial_value)\n",
    "#         record biases\n",
    "        tf.histogram_summary('biases'+str(i),biases)\n",
    "        \n",
    "        outputs = tf.nn.relu(tf.matmul(inputs,weights)+biases)\n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs contain batchsize * oneImage of data\n",
    "# inputs = tf.placeholder(dtype='float32',shape=(None,mnist.IMAGE_PIXELS))\n",
    "\n",
    "def inference(images,fcn):\n",
    "#     fcn : num of units in each layers using `[,]` format except input and output layer \n",
    "#     outputs=None\n",
    "\n",
    "    insize = mnist.IMAGE_PIXELS\n",
    "    inputs = images\n",
    "    \n",
    "    for i in range(len(fcn)):\n",
    "        \n",
    "        outputs = onelayer(\n",
    "            inputs,'fcn'+str(i),\n",
    "            tf.truncated_normal([insize, fcn[i]],stddev=1.0 / math.sqrt(float(insize))),\n",
    "            tf.zeros(fcn[i]),i)\n",
    "        tf.histogram_summary('fcn'+str(i)+'/activations',outputs)\n",
    "#        last layer's outputs is the next layer's inputs\n",
    "        inputs = outputs\n",
    "        insize = fcn[i]\n",
    "#     get logit(probability of each class)\n",
    "# num of classes is 10 for mnist\n",
    "    logits = onelayer(\n",
    "    inputs,'fcn'+str(i+1),\n",
    "    tf.truncated_normal([insize, 10],stddev=1.0 / math.sqrt(float(insize))),\n",
    "    tf.zeros(10),i+1)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getloss(logits,labels):\n",
    "#     softmax loss function\n",
    "# logits is [batch_size,num_classes] tensor and labels is [batch_size]\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy,name='xentropy_mean')\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss,learning_rate):\n",
    "\n",
    "    \n",
    "#     add summary\n",
    "#     tf.summary.scalar('loss', loss)\n",
    "    tf.scalar_summary('loss',loss)\n",
    "#     create optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     global_step \n",
    "    global_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "#     minimize the loss \n",
    "    train_op = optimizer.minimize(loss,global_step = global_step)\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits,labels):\n",
    "#     correct is a bool tensor\n",
    "# return a scalar int32 tensor\n",
    "    correct = tf.nn.in_top_k(logits,labels,1)\n",
    "#     get error rate in one batch\n",
    "    return tf.reduce_sum(tf.cast(correct,tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def solve(datasets,fcn):\n",
    "#     #     init\n",
    "#     logits = inference(inputs)\n",
    "    \n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create placeholder for images and labels at the number of batch_size\n",
    "def placeholder_inputs(batch_size):\n",
    "#     couvert dataset to images and labels\n",
    "    images_placeholder = tf.placeholder(tf.float32,shape=(batch_size,mnist.IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    \n",
    "    return images_placeholder,labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then create method for feed placeholder\n",
    "def fill_feed_dict(dataset,images_pl,labels_pl,batch_size):\n",
    "#     dataset refers to train or test dataset\n",
    "\n",
    "# feed bacth_size numbers of images and labels \n",
    "    images_feed,labels_feed = dataset.next_batch(batch_size)\n",
    "    \n",
    "    feed_dict = {\n",
    "        images_pl:images_feed,\n",
    "        labels_pl:labels_feed,\n",
    "    }\n",
    "    \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "           eval_correct,\n",
    "           images_placeholder,\n",
    "           labels_placeholder,\n",
    "           data_set,batch_size):\n",
    "#     run test in test_data\n",
    "    true_count = 0\n",
    "    steps_per_epoch = data_set.num_examples // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,\n",
    "                                  images_placeholder,\n",
    "                                  labels_placeholder)\n",
    "        true_count += sess.run(eval_correct,feed_dict=feed_dict)\n",
    "    precision = true_count / num_examples\n",
    "    print(' Num examples: %d Num correct: %d Precision @ 1: %0.04f' %\n",
    "         (num_examples,true_count,precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dir = 'dataset/mnist'\n",
    "summary_dir = 'dataset/mnist/summary'\n",
    "def run_training(batch_size,fcn,learning_rate,iters):\n",
    "    \n",
    "    # the datasets include traindata and testdata,\n",
    "    # so get traindata by using datasets.train for example.\n",
    "    # And get other dataset like before .\n",
    "    datasets = input_data.read_data_sets(train_dir=train_dir)\n",
    "    with tf.Graph().as_default():\n",
    "#         generate placeholders for images and labels\n",
    "        images_placeholder,labels_placeholder = placeholder_inputs(\n",
    "        batch_size)\n",
    "        \n",
    "#       build a Graph\n",
    "        logits = inference(images_placeholder,fcn)\n",
    "        \n",
    "#         add loss option\n",
    "        loss = getloss(logits,labels_placeholder)\n",
    "        \n",
    "#         tf.scalar_summary('loss',loss)\n",
    "#         Add to the Graph the Ops that calculate and apply gradients\n",
    "        train_op = training(loss,learning_rate)\n",
    "        \n",
    "#         Add the Op to compare the logits to the labels during evaluation\n",
    "        eval_correct = evaluation(logits,labels_placeholder)\n",
    "        \n",
    "#         build the summary \n",
    "        summary = tf.merge_all_summaries()\n",
    "        \n",
    "#         initialize all variable\n",
    "        init = tf.initialize_all_variables()\n",
    "        \n",
    "#         Create a saver for writing training checkpoints\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "#         Create a session for running Ops on the Graph\n",
    "        sess = tf.Session()\n",
    "        \n",
    "#         Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "        summary_writer = tf.train.SummaryWriter(summary_dir,sess.graph)\n",
    "        \n",
    "#         run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "    \n",
    "#         Start the training loop.\n",
    "        for step in range(iters):\n",
    "            start_time = time.time()\n",
    "            \n",
    "#             feed data to placeholders(train_data)\n",
    "            feed_dict = fill_feed_dict(datasets.train,\n",
    "                                       images_placeholder,\n",
    "                                       labels_placeholder,\n",
    "                                      batch_size)\n",
    "#             get the return value (tensor from the Graph)\n",
    "            _, loss_value = sess.run([train_op,loss],\n",
    "                                    feed_dict=feed_dict)\n",
    "            \n",
    "#             get the duration\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "#         Add the summary often\n",
    "            if(step%100 ==0):\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step,loss_value,duration))\n",
    "                \n",
    "#                 update the events file.\n",
    "                summary_str = sess.run(summary,\n",
    "                                       feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str,step)\n",
    "                summary_writer.flush()\n",
    "                \n",
    "#                 Save a checkpoint and evaluate the model periodically\n",
    "                if(step + 1)% 1000 == 0 or (step + 1)==iters:\n",
    "#                     set dir to save\n",
    "                    checkpoint_file = os.path.join(summary_dir,'checkpoint')\n",
    "                    saver.save(sess,checkpoint_file,global_step = step)\n",
    "                    \n",
    "#                     evaluate against the training set.\n",
    "                    print('Validation Data Eval:')\n",
    "                    do_eval(sess,\n",
    "                           eval_correct,\n",
    "                           images_placeholder,\n",
    "                           labels_placeholder,\n",
    "                           datasets.validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting dataset/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.30 (0.022 sec)\n",
      "Step 100: loss = 2.30 (0.002 sec)\n",
      "Step 200: loss = 2.30 (0.002 sec)\n",
      "Step 300: loss = 2.30 (0.005 sec)\n",
      "Step 400: loss = 2.30 (0.003 sec)\n",
      "Step 500: loss = 2.30 (0.002 sec)\n",
      "Step 600: loss = 2.30 (0.002 sec)\n",
      "Step 700: loss = 2.30 (0.007 sec)\n",
      "Step 800: loss = 2.29 (0.007 sec)\n",
      "Step 900: loss = 2.29 (0.002 sec)\n",
      "Step 1000: loss = 2.28 (0.002 sec)\n",
      "Step 1100: loss = 2.26 (0.051 sec)\n",
      "Step 1200: loss = 2.24 (0.002 sec)\n",
      "Step 1300: loss = 2.15 (0.003 sec)\n",
      "Step 1400: loss = 2.15 (0.004 sec)\n",
      "Step 1500: loss = 2.09 (0.004 sec)\n",
      "Step 1600: loss = 2.03 (0.002 sec)\n",
      "Step 1700: loss = 2.00 (0.002 sec)\n",
      "Step 1800: loss = 2.00 (0.003 sec)\n",
      "Step 1900: loss = 1.99 (0.002 sec)\n",
      "Step 2000: loss = 1.97 (0.003 sec)\n",
      "Step 2100: loss = 1.87 (0.002 sec)\n",
      "Step 2200: loss = 1.82 (0.050 sec)\n",
      "Step 2300: loss = 1.71 (0.002 sec)\n",
      "Step 2400: loss = 1.66 (0.002 sec)\n",
      "Step 2500: loss = 1.67 (0.003 sec)\n",
      "Step 2600: loss = 1.43 (0.002 sec)\n",
      "Step 2700: loss = 1.76 (0.004 sec)\n",
      "Step 2800: loss = 1.29 (0.002 sec)\n",
      "Step 2900: loss = 1.27 (0.003 sec)\n",
      "Step 3000: loss = 1.14 (0.002 sec)\n",
      "Step 3100: loss = 0.88 (0.005 sec)\n",
      "Step 3200: loss = 0.95 (0.002 sec)\n",
      "Step 3300: loss = 1.03 (0.065 sec)\n",
      "Step 3400: loss = 0.76 (0.003 sec)\n",
      "Step 3500: loss = 0.83 (0.003 sec)\n",
      "Step 3600: loss = 0.86 (0.002 sec)\n",
      "Step 3700: loss = 0.77 (0.004 sec)\n",
      "Step 3800: loss = 0.96 (0.002 sec)\n",
      "Step 3900: loss = 0.58 (0.004 sec)\n",
      "Step 4000: loss = 0.77 (0.002 sec)\n",
      "Step 4100: loss = 0.69 (0.003 sec)\n",
      "Step 4200: loss = 0.89 (0.003 sec)\n",
      "Step 4300: loss = 0.77 (0.002 sec)\n",
      "Step 4400: loss = 0.68 (0.069 sec)\n",
      "Step 4500: loss = 0.72 (0.003 sec)\n",
      "Step 4600: loss = 0.70 (0.002 sec)\n",
      "Step 4700: loss = 0.97 (0.004 sec)\n",
      "Step 4800: loss = 0.67 (0.002 sec)\n",
      "Step 4900: loss = 0.80 (0.002 sec)\n",
      "Step 5000: loss = 0.55 (0.003 sec)\n",
      "Step 5100: loss = 0.69 (0.002 sec)\n",
      "Step 5200: loss = 0.48 (0.002 sec)\n",
      "Step 5300: loss = 0.51 (0.003 sec)\n",
      "Step 5400: loss = 0.53 (0.002 sec)\n",
      "Step 5500: loss = 0.42 (0.057 sec)\n",
      "Step 5600: loss = 0.52 (0.002 sec)\n",
      "Step 5700: loss = 0.59 (0.002 sec)\n",
      "Step 5800: loss = 0.66 (0.002 sec)\n",
      "Step 5900: loss = 0.49 (0.002 sec)\n",
      "Step 6000: loss = 0.62 (0.002 sec)\n",
      "Step 6100: loss = 0.43 (0.002 sec)\n",
      "Step 6200: loss = 0.61 (0.002 sec)\n",
      "Step 6300: loss = 0.61 (0.002 sec)\n",
      "Step 6400: loss = 0.63 (0.002 sec)\n",
      "Step 6500: loss = 0.51 (0.002 sec)\n",
      "Step 6600: loss = 0.64 (0.049 sec)\n",
      "Step 6700: loss = 0.44 (0.002 sec)\n",
      "Step 6800: loss = 0.47 (0.002 sec)\n",
      "Step 6900: loss = 0.45 (0.002 sec)\n",
      "Step 7000: loss = 0.37 (0.002 sec)\n",
      "Step 7100: loss = 0.56 (0.002 sec)\n",
      "Step 7200: loss = 0.60 (0.002 sec)\n",
      "Step 7300: loss = 0.39 (0.002 sec)\n",
      "Step 7400: loss = 0.45 (0.002 sec)\n",
      "Step 7500: loss = 0.54 (0.002 sec)\n",
      "Step 7600: loss = 0.63 (0.002 sec)\n",
      "Step 7700: loss = 0.53 (0.051 sec)\n",
      "Step 7800: loss = 0.41 (0.016 sec)\n",
      "Step 7900: loss = 0.48 (0.002 sec)\n",
      "Step 8000: loss = 0.44 (0.002 sec)\n",
      "Step 8100: loss = 0.48 (0.002 sec)\n",
      "Step 8200: loss = 0.35 (0.002 sec)\n",
      "Step 8300: loss = 0.73 (0.002 sec)\n",
      "Step 8400: loss = 0.45 (0.002 sec)\n",
      "Step 8500: loss = 0.51 (0.003 sec)\n",
      "Step 8600: loss = 0.27 (0.002 sec)\n",
      "Step 8700: loss = 0.50 (0.002 sec)\n",
      "Step 8800: loss = 0.26 (0.066 sec)\n",
      "Step 8900: loss = 0.59 (0.002 sec)\n",
      "Step 9000: loss = 0.40 (0.002 sec)\n",
      "Step 9100: loss = 0.51 (0.002 sec)\n",
      "Step 9200: loss = 0.51 (0.002 sec)\n",
      "Step 9300: loss = 0.43 (0.002 sec)\n",
      "Step 9400: loss = 0.33 (0.002 sec)\n",
      "Step 9500: loss = 0.30 (0.002 sec)\n",
      "Step 9600: loss = 0.48 (0.002 sec)\n",
      "Step 9700: loss = 0.72 (0.003 sec)\n",
      "Step 9800: loss = 0.43 (0.002 sec)\n",
      "Step 9900: loss = 0.34 (0.058 sec)\n"
     ]
    }
   ],
   "source": [
    "fcn = [12,24,24,32,32,32,32,128,128]\n",
    "learning_rate = 0.01\n",
    "iters = 10000\n",
    "run_training(batch_size=100,fcn=fcn,learning_rate=learning_rate,iters=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.contrib.layers.fully_connected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### another practice for getting gradients of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_size= 10 \n",
    "out_size=  10\n",
    "x = tf.placeholder(tf.float32,shape=[None,in_size])\n",
    "w = tf.Variable(tf.truncated_normal([in_size,out_size],stddev=1e-5),name='weights')\n",
    "b = tf.Variable(tf.zeros([out_size]))\n",
    "output = tf.matmul(x, w) + b\n",
    "\n",
    "# calculate the loss between y and output\n",
    "labels = tf.placeholder(tf.int32,shape=[None])\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "output,labels,name='xentropy')\n",
    "loss = tf.reduce_mean(cross_entropy,name='xentropy_mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the gradient the \n",
    "gradient_w = tf.gradients(output,w)\n",
    "gradient_b = tf.gradients(output,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "high is out of bounds for int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-2875b71ef455>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m feed_dict = {\n\u001b[0;32m     17\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     20\u001b[0m loss_value = sess.run(loss,\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randint (numpy/random/mtrand/mtrand.c:14458)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: high is out of bounds for int64"
     ]
    }
   ],
   "source": [
    "# add summary of gradient of w and b\n",
    "tf.histogram_summary('gradient_w',gradient_w)\n",
    "tf.histogram_summary('gradient_b',gradient_b)\n",
    "tf.scalar_summary('loss',loss)\n",
    "\n",
    "summary_dir = 'summary'\n",
    "summary = tf.merge_all_summaries()\n",
    "summary_writer = tf.train.SummaryWriter(summary_dir,sess.graph)\n",
    "# run the session \n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# feed the data\n",
    "# [batch * in_size]\n",
    "data = np.random.random([5,10])\n",
    "feed_dict = {\n",
    "    x:data,\n",
    "    labels:np.random.randint([5]),\n",
    "}\n",
    "loss_value = sess.run(loss,\n",
    "         feed_dict=feed_dict)\n",
    "\n",
    "# get summary and display\n",
    "# summary_str = sess.run(summary,\n",
    "#                        feed_dict={x:data})\n",
    "# record one item\n",
    "summary_writer.add_summary(summary_str,1)\n",
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(gradient_b,feed_dict={x:data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
